{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler, AutoPipelineForInpainting, StableDiffusionPipeline\n",
    "\n",
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageWidth = 512    \n",
    "imageHeight = 512                                         \n",
    "guidance_scale = 8                                                             \n",
    "model_path = \"segmind/tiny-sd\" \n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(torch_device)\n",
    "scheduler = LMSDiscreteScheduler.from_pretrained(model_path, subfolder=\"scheduler\")\n",
    "unet = UNet2DConditionModel.from_pretrained(model_path, subfolder=\"unet\").to(torch_device)\n",
    "vae = AutoencoderKL.from_pretrained(model_path, subfolder=\"vae\").to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline(\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=text_encoder,\n",
    "    unet=unet,\n",
    "    scheduler=scheduler,\n",
    "    vae=vae,  \n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    "    requires_safety_checker=False,\n",
    "    ).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae\\diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383edaab00a64055a7d39ee0874e7e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inpaint_pipe = AutoPipelineForInpainting.from_pipe(pipe).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dict, seed, prompt=\"\"):\n",
    "    mask = dict[\"layers\"][0].convert(\"RGB\").resize((imageHeight, imageWidth))\n",
    "    init_image = dict[\"background\"].convert(\"RGB\").resize((imageHeight, imageWidth))\n",
    "    output = inpaint_pipe(prompt = prompt, image=init_image, mask_image=mask, guidance_scale=guidance_scale, generator=torch.Generator().manual_seed(seed))\n",
    "    return output.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4957f100264ca38266792626923464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image = gr.Sketchpad(sources='upload', type=\"pil\", label=\"Upload\", brush=gr.Brush(colors=[\"#ffff00\"]))\n",
    "        with gr.Column():\n",
    "            image_out = gr.Image(label=\"Output\")\n",
    "\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(label=\"Prompt\")\n",
    "        seed = gr.Number(label=\"Seed\", value=0)\n",
    "        btn = gr.Button(\"Inpaint\")\n",
    "\n",
    "    btn.click(fn=predict, inputs=[image, seed, prompt], outputs=[image_out])\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
